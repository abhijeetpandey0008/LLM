{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -qU langgraph langchain-core langchain langchain-openai langchain-community google-api-python-client\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L2Gs41a_9H-",
        "outputId": "2ce74355-1f91-4ba1-85f3-96190d036770"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.4/438.4 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai\n",
        "\n",
        "# --- REQUIRED IMPORTS ---\n",
        "import os\n",
        "import re\n",
        "from typing import List, Dict, Any, TypedDict\n",
        "\n",
        "# Langchain core components\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# Langgraph specific\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Real LLM integration (using Google Generative AI as an example)\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI # <<< CHANGED THIS LINE\n",
        "\n",
        "# Real Search Tool integration (using Google Custom Search)\n",
        "from langchain_community.tools import GoogleSearchRun\n",
        "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
        "\n",
        "# Agent Executor for ToolAgent\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "try:\n",
        "    # For Google Colab Secrets\n",
        "    from google.colab import userdata\n",
        "    # os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY') # <<< REMOVED/COMMENTED OUT THIS LINE\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "    os.environ[\"GOOGLE_CSE_ID\"] = userdata.get('GOOGLE_CSE_ID')\n",
        "    print(\"API keys loaded from Google Colab secrets.\")\n",
        "except ImportError:\n",
        "    # For local environment, ensure these are set as environment variables\n",
        "    print(\"Not in Colab. Ensure GOOGLE_API_KEY and GOOGLE_CSE_ID are set as environment variables.\")\n",
        "    if not os.getenv(\"GOOGLE_API_KEY\") or not os.getenv(\"GOOGLE_CSE_ID\"):\n",
        "        raise ValueError(\"API keys for Google API or Google CSE ID are not set. \"\n",
        "                         \"Please set them as environment variables or Colab secrets.\")\n",
        "\n",
        "\n",
        "# --- 1. DEFINE THE GRAPH STATE ---\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        user_query (str): The original user query.\n",
        "        sub_tasks (List[Dict[str, Any]]): A list of sub-tasks generated by the PlanAgent.\n",
        "                                          Each sub-task can have fields like 'description', 'status', 'result'.\n",
        "        current_task_index (int): The index of the sub-task currently being processed.\n",
        "        tools (List[str]): List of tool names available to the ToolAgent.\n",
        "        reflection (str): Feedback or reflection from the Inner Loop to the PlanAgent.\n",
        "        final_result (str): The accumulated final result of all sub-tasks.\n",
        "        iteration_count (int): To keep track of the refinement iterations.\n",
        "    \"\"\"\n",
        "    user_query: str\n",
        "    sub_tasks: List[Dict[str, Any]]\n",
        "    current_task_index: int\n",
        "    tools: List[str]\n",
        "    reflection: str\n",
        "    final_result: str\n",
        "    iteration_count: int\n",
        "\n",
        "\n",
        "# --- 2. INITIALIZE REAL LANGUAGE MODELS AND TOOLS ---\n",
        "\n",
        "# Initialize a real LLM (using Google's Gemini-Pro) <<< CHANGED THIS LINE\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0) # <<< CHANGED THIS LINE\n",
        "\n",
        "# Define real tools\n",
        "# Google Search Tool\n",
        "search_wrapper = GoogleSearchAPIWrapper()\n",
        "Google_Search_tool = GoogleSearchRun(api_wrapper=search_wrapper)\n",
        "\n",
        "# Simple Calculator Tool (be cautious with eval() in production for security)\n",
        "@tool\n",
        "def calculator_tool(expression: str) -> str:\n",
        "    \"\"\"Evaluates a mathematical expression.\"\"\"\n",
        "    print(f\"Executing calculation for: {expression}\")\n",
        "    try:\n",
        "        return str(eval(expression))\n",
        "    except Exception as e:\n",
        "        return f\"Error evaluating expression: {e}\"\n",
        "\n",
        "# List of actual tools available to the ToolAgent\n",
        "tools = [Google_Search_tool, calculator_tool]\n",
        "# The names in the tools list here are the actual tool function objects,\n",
        "# which the AgentExecutor will use.\n",
        "\n",
        "\n",
        "# --- 3. IMPLEMENT THE PLANAGENT NODE ---\n",
        "def plan_agent_node(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    PlanAgent: Splits the user query into sub-tasks or refines existing sub-tasks based on reflection.\n",
        "    Uses a real LLM for planning.\n",
        "    \"\"\"\n",
        "    user_query = state[\"user_query\"]\n",
        "    sub_tasks = state[\"sub_tasks\"]\n",
        "    reflection = state[\"reflection\"]\n",
        "    iteration_count = state[\"iteration_count\"]\n",
        "\n",
        "    print(f\"\\n--- PlanAgent (Iteration {iteration_count}) ---\")\n",
        "    print(f\"User Query: {user_query}\")\n",
        "    print(f\"Existing Sub-tasks: {sub_tasks}\")\n",
        "    print(f\"Reflection from previous step: {reflection}\")\n",
        "\n",
        "    prompt_messages = [\n",
        "        (\"system\", \"\"\"You are a PlanAgent. Your goal is to break down complex user queries into smaller, manageable sub-tasks.\n",
        "         Each sub-task should be clear, actionable, and ideally solvable by a specialized tool (like search or calculator).\n",
        "         If 'reflection' is provided, refine the existing sub-tasks (modify, delete, or add new ones) based on the feedback.\n",
        "         Always output a JSON array of sub-tasks. Each sub-task *must* have a 'description' and a unique 'id' (e.g., \"task_1\").\n",
        "\n",
        "         If refining (based on reflection), you can also include an 'action' key:\n",
        "         - {{\"id\": \"task_X\", \"action\": \"delete\"}} to remove a task.\n",
        "         - {{\"id\": \"task_Y\", \"description\": \"New task\", \"action\": \"add\"}} to add a new task.\n",
        "         - {{\"id\": \"task_Z\", \"description\": \"Modified task\"}} to update an existing task.\n",
        "\n",
        "         If no sub-tasks are needed (e.g., a simple direct question that can be answered immediately), return an empty array [].\n",
        "         \"\"\"),\n",
        "        (\"user\", \"User Query: {user_query}\\nExisting Sub-tasks: {sub_tasks}\\nReflection: {reflection}\\nIteration Count: {iteration_count}\")\n",
        "    ]\n",
        "\n",
        "    prompt_template = ChatPromptTemplate.from_messages(prompt_messages)\n",
        "    parser = JsonOutputParser()\n",
        "    chain = prompt_template | llm | parser\n",
        "\n",
        "    try:\n",
        "        response = chain.invoke({\"user_query\": user_query, \"sub_tasks\": sub_tasks, \"reflection\": reflection, \"iteration_count\": iteration_count})\n",
        "        if not isinstance(response, list):\n",
        "            print(f\"Warning: PlanAgent response not a list: {response}. Attempting to parse.\")\n",
        "            response = [] # Fallback\n",
        "        new_tasks_from_plan = response\n",
        "    except Exception as e:\n",
        "        print(f\"Error invoking PlanAgent LLM: {e}. Falling back to default plan.\")\n",
        "        new_tasks_from_plan = [{\"id\": \"task_1\", \"description\": f\"Address the query: {user_query}\"}]\n",
        "\n",
        "\n",
        "    # Apply refinements (add/delete/modify) from the LLM's response\n",
        "    updated_sub_tasks_dict = {task.get('id'): task for task in sub_tasks if task.get('action') != 'delete'}\n",
        "\n",
        "    task_id_counter = 0\n",
        "    if updated_sub_tasks_dict:\n",
        "        # Find max existing id to ensure new unique IDs\n",
        "        max_id_num = 0\n",
        "        for task_id in updated_sub_tasks_dict.keys():\n",
        "            if task_id.startswith(\"task_\") and task_id[5:].isdigit():\n",
        "                max_id_num = max(max_id_num, int(task_id[5:]))\n",
        "        task_id_counter = max_id_num\n",
        "\n",
        "    final_sub_tasks_list = []\n",
        "    for task_def in new_tasks_from_plan:\n",
        "        if task_def.get(\"action\") == \"delete\":\n",
        "            if task_def.get(\"id\") in updated_sub_tasks_dict:\n",
        "                del updated_sub_tasks_dict[task_def.get(\"id\")]\n",
        "            continue\n",
        "        elif task_def.get(\"action\") == \"add\":\n",
        "            task_id_counter += 1\n",
        "            new_id = f\"task_{task_id_counter}\"\n",
        "            task_def[\"id\"] = new_id\n",
        "            final_sub_tasks_list.append({k: v for k, v in task_def.items() if k != \"action\"})\n",
        "        else: # Modify or new task without explicit \"add\"\n",
        "            # Ensure unique IDs for newly generated tasks if LLM doesn't provide them unique\n",
        "            if \"id\" not in task_def or task_def[\"id\"] not in updated_sub_tasks_dict:\n",
        "                 task_id_counter += 1\n",
        "                 task_def[\"id\"] = f\"task_{task_id_counter}\"\n",
        "\n",
        "            final_sub_tasks_list.append({k: v for k, v in task_def.items() if k != \"action\"})\n",
        "\n",
        "    # Ensure all tasks from the original set that weren't deleted or explicitly modified are carried forward\n",
        "    # This might need more sophisticated merging if LLM returns a full new plan vs. incremental changes\n",
        "    # For simplicity, we'll take the LLM's latest full plan if it's not empty, otherwise merge.\n",
        "    if new_tasks_from_plan:\n",
        "        new_sub_tasks = final_sub_tasks_list # Trust the LLM's latest full plan\n",
        "    else:\n",
        "        new_sub_tasks = list(updated_sub_tasks_dict.values()) # If LLM returned empty, use existing tasks (if not deleted)\n",
        "\n",
        "\n",
        "    state[\"sub_tasks\"] = new_sub_tasks\n",
        "    state[\"current_task_index\"] = 0  # Reset to the first task if planning/refinement occurs\n",
        "    state[\"reflection\"] = \"\"  # Clear reflection after it's used\n",
        "    state[\"iteration_count\"] += 1\n",
        "    print(f\"PlanAgent generated/updated sub-tasks: {state['sub_tasks']}\")\n",
        "    return state\n",
        "\n",
        "\n",
        "#  IMPLEMENT THE TOOLAGENT NODE\n",
        "def tool_agent_node(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    ToolAgent: Executes the current sub-task using available tools.\n",
        "    Uses a real LLM with AgentExecutor for tool selection and execution.\n",
        "    \"\"\"\n",
        "    current_task_index = state[\"current_task_index\"]\n",
        "    sub_tasks = state[\"sub_tasks\"]\n",
        "\n",
        "    if current_task_index >= len(sub_tasks):\n",
        "        print(\"ToolAgent: No more tasks to execute.\")\n",
        "        return state\n",
        "\n",
        "    current_task = sub_tasks[current_task_index]\n",
        "    task_description = current_task[\"description\"]\n",
        "\n",
        "    print(f\"\\n--- ToolAgent ---\")\n",
        "    print(f\"Executing task ({current_task_index + 1}/{len(sub_tasks)}): {task_description}\")\n",
        "\n",
        "    # Define the tool agent prompt\n",
        "    agent_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful assistant. Use the provided tools to execute the given task. \"\n",
        "                   \"If the task requires a search, use the 'Google Search_tool'. \"\n",
        "                   \"If it requires a calculation, use the 'calculator_tool'. \"\n",
        "                   \"If you have the final answer, output it directly. Do not make up information.\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"placeholder\", \"{agent_scratchpad}\") # This is where the agent's thoughts and tool calls go\n",
        "    ])\n",
        "\n",
        "    # Create the tool-calling agent\n",
        "    agent = create_tool_calling_agent(llm, tools, agent_prompt)\n",
        "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
        "\n",
        "    task_result = \"\"\n",
        "    try:\n",
        "        # Invoke the agent executor with the current task description\n",
        "        response = agent_executor.invoke({\"input\": task_description})\n",
        "        task_result = response.get(\"output\", \"No direct output from agent executor, check verbose logs.\")\n",
        "        if task_result.strip() == \"\":\n",
        "            task_result = f\"Agent executed task '{task_description}' but returned no explicit output. Check logs.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        task_result = f\"Error during tool execution for task '{task_description}': {e}\"\n",
        "        print(f\"Error in ToolAgent execution: {e}\")\n",
        "\n",
        "    state[\"sub_tasks\"][current_task_index][\"result\"] = task_result\n",
        "    state[\"sub_tasks\"][current_task_index][\"status\"] = \"completed\"\n",
        "    state[\"current_task_index\"] += 1\n",
        "    state[\"final_result\"] += f\"\\n- {task_description}: {task_result}\"\n",
        "    print(f\"Task result: {task_result}\")\n",
        "    return state\n",
        "\n",
        "\n",
        "# IMPLEMENT REFLECTION NODE\n",
        "def reflection_node(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    Reflection: Evaluates the outcome of the ToolAgent's execution and generates feedback for PlanAgent.\n",
        "    Uses a real LLM for reflection.\n",
        "    \"\"\"\n",
        "    current_task_index = state[\"current_task_index\"]\n",
        "    sub_tasks = state[\"sub_tasks\"]\n",
        "    user_query = state[\"user_query\"]\n",
        "\n",
        "    print(f\"\\n--- Reflection Node ---\")\n",
        "\n",
        "    # Determine if all planned tasks have been processed by ToolAgent\n",
        "    if current_task_index >= len(sub_tasks):\n",
        "        state[\"reflection\"] = \"All planned tasks processed. Ready for final review.\"\n",
        "        print(\"Reflection: All planned tasks processed.\")\n",
        "        return state\n",
        "\n",
        "    # Otherwise, reflect on the last completed task\n",
        "    previous_task_index = current_task_index - 1\n",
        "    reflection_feedback = \"No specific feedback.\" # Default\n",
        "    if previous_task_index >= 0:\n",
        "        previous_task = sub_tasks[previous_task_index]\n",
        "        task_result = previous_task.get(\"result\", \"\")\n",
        "        task_description = previous_task.get(\"description\", \"unknown task\")\n",
        "        task_status = previous_task.get(\"status\", \"unknown\")\n",
        "\n",
        "        reflection_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a Reflection Agent. Your job is to analyze the outcome of a completed task and provide feedback.\n",
        "             Determine if the task was completed successfully, if it failed, or if it needs more information/refinement.\n",
        "             If it failed or needs refinement, provide specific instructions for the PlanAgent to modify, delete, or add tasks.\n",
        "             If successful, simply state that.\n",
        "             Example feedback for refinement: \"Task 'Research X' failed. The result was empty. Please refine the task to 'Search for X with alternative keywords'.\"\n",
        "             Example for success: \"Task 'Calculate Y' completed successfully.\"\n",
        "             \"\"\"),\n",
        "            (\"user\", f\"Original Query: {user_query}\\nTask Description: {task_description}\\nTask Result: {task_result}\\nTask Status: {task_status}\")\n",
        "        ])\n",
        "\n",
        "        chain = reflection_prompt | llm\n",
        "        try:\n",
        "            reflection_feedback = chain.invoke({\"user_query\": user_query, \"task_description\": task_description, \"task_result\": task_result, \"task_status\": task_status}).content\n",
        "        except Exception as e:\n",
        "            reflection_feedback = f\"Error during reflection: {e}. Assuming task '{task_description}' needs review.\"\n",
        "\n",
        "        print(f\"Reflection for '{task_description}': {reflection_feedback}\")\n",
        "    else:\n",
        "        print(\"Reflection: Initial reflection, no tasks processed yet.\")\n",
        "\n",
        "    state[\"reflection\"] = reflection_feedback\n",
        "    return state\n",
        "\n",
        "\n",
        "# IMPLEMENT AGENT DISPATCH (CONDITIONAL EDGE LOGIC)\n",
        "\n",
        "def should_continue_planning(state: GraphState) -> str:\n",
        "    \"\"\"\n",
        "    Decides whether to continue with planning/refinement (Outer Loop)\n",
        "    or move to task execution (Inner Loop), or finish.\n",
        "    This is called FROM the 'reflection' node.\n",
        "    \"\"\"\n",
        "    sub_tasks = state[\"sub_tasks\"]\n",
        "    current_task_index = state[\"current_task_index\"]\n",
        "    reflection = state[\"reflection\"]\n",
        "\n",
        "    print(f\"\\n--- Deciding Next Step from Reflection ---\")\n",
        "    print(f\"Current reflection: '{reflection}'\")\n",
        "    print(f\"Current task index: {current_task_index}, Total tasks: {len(sub_tasks)}\")\n",
        "\n",
        "    # If reflection indicates a need for refinement (e.g., a task failed)\n",
        "    if \"refine the plan\" in reflection.lower() or \"failed\" in reflection.lower() or \"needs more information\" in reflection.lower():\n",
        "        print(\"Decision: Go back to PlanAgent for replanning/refinement.\")\n",
        "        return \"replan\"\n",
        "\n",
        "    # If all current tasks in the list have been processed by ToolAgent and the reflection confirms this, then the overall workflow is done.\n",
        "\n",
        "    if current_task_index >= len(sub_tasks) and \"all planned tasks processed\" in reflection.lower():\n",
        "        print(\"Decision: All tasks completed. END workflow.\")\n",
        "        return \"end\"\n",
        "\n",
        "    # If there are still tasks in the current plan that haven't been processed yet\n",
        "    if current_task_index < len(sub_tasks):\n",
        "        print(\"Decision: Continue to ToolAgent for next task execution.\")\n",
        "        return \"continue\"\n",
        "\n",
        "    # Fallback: If somehow the plan is empty or in an unexpected state after reflection\n",
        "    print(\"Decision: Defaulting to replan (unexpected state).\")\n",
        "    return \"replan\"\n",
        "\n",
        "def should_continue_task_execution(state: GraphState) -> str:\n",
        "    \"\"\"\n",
        "    Decides whether to immediately execute tasks or go to reflection.\n",
        "    This is called FROM the 'plan_agent' node.\n",
        "    \"\"\"\n",
        "    sub_tasks = state[\"sub_tasks\"]\n",
        "    current_task_index = state[\"current_task_index\"]\n",
        "\n",
        "    print(f\"\\n Deciding Next Step from PlanAgent\")\n",
        "    print(f\"Tasks planned: {len(sub_tasks)}, Current task index: {current_task_index}\")\n",
        "\n",
        "    # If the PlanAgent has generated tasks and there are tasks to execute\n",
        "    if sub_tasks and current_task_index < len(sub_tasks):\n",
        "        print(\"Decision: Tasks available. Continue to ToolAgent for execution.\")\n",
        "        return \"continue\" # More tasks to execute\n",
        "    else:\n",
        "        # If the plan is empty ( LLM decided no tasks needed, or initial plan is empty)\n",
        "        print(\"Decision: No tasks from PlanAgent or plan is empty. Go to Reflection.\")\n",
        "        return \"reflect\" # Go to reflection_node  to handle empty plan or final review\n",
        "\n",
        "#  BUILD THE LANGGRAPH GRAPH\n",
        "\n",
        "# Create the graph\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add nodes for each agent/component\n",
        "workflow.add_node(\"plan_agent\", plan_agent_node)\n",
        "workflow.add_node(\"tool_agent\", tool_agent_node)\n",
        "workflow.add_node(\"reflection_node\", reflection_node)\n",
        "\n",
        "# Set the entry point for the graph\n",
        "workflow.set_entry_point(\"plan_agent\")\n",
        "# From 'plan_agent': Decide whether to execute tasks or reflect (if plan is empty)\n",
        "workflow.add_conditional_edges(\n",
        "    \"plan_agent\",\n",
        "    should_continue_task_execution,\n",
        "    {\n",
        "        \"continue\": \"tool_agent\",  # If tasks are ready, go to tool agent\n",
        "        \"reflect\": \"reflection_node\"    # If no tasks from plan, go to reflection\n",
        "    }\n",
        ")\n",
        "\n",
        "# From 'tool_agent': Always go to reflection after executing a task\n",
        "workflow.add_edge(\"tool_agent\", \"reflection_node\")\n",
        "\n",
        "# From 'reflection_node': Decide whether to replan, continue tasks, or end the workflow\n",
        "workflow.add_conditional_edges(\n",
        "    \"reflection_node\",\n",
        "    should_continue_planning,\n",
        "    {\n",
        "        \"replan\": \"plan_agent\", # Go back to PlanAgent for refinement\n",
        "        \"continue\": \"tool_agent\", # Continue with next task in Inner Loop (if reflection allows)\n",
        "        \"end\": END              # it will  terminate the workflow once all is doen\n",
        "    }\n",
        ")\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "def run_agent_workflow_from_user_input():\n",
        "    \"\"\"\n",
        "    Function to run the agentic workflow by taking continuous user input.\n",
        "    \"\"\"\n",
        "    print(\"--- Agentic Workflow powered by Langgraph (Real LLM & Tools) ---\")\n",
        "    print(\"Enter your queries below. Type 'exit' to quit.\")\n",
        "\n",
        "    while True:\n",
        "        user_input_query = input(\"\\nEnter your query: \").strip()\n",
        "        if user_input_query.lower() == 'exit':\n",
        "            print(\"Exiting workflow. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if not user_input_query:\n",
        "            print(\"Query cannot be empty. Please enter a query.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n[SYSTEM] Processing query: '{user_input_query}'\")\n",
        "\n",
        "        # Initialize the state with the user's query for each new interaction\n",
        "        initial_state = GraphState(\n",
        "            user_query=user_input_query,\n",
        "            sub_tasks=[],\n",
        "            current_task_index=0,\n",
        "            tools=[\"Google Search_tool\", \"calculator_tool\"], # Names of the tools for conceptual tracking\n",
        "            reflection=\"\",\n",
        "            final_result=\"\",\n",
        "            iteration_count=0\n",
        "        )\n",
        "\n",
        "        try:\n",
        "\n",
        "            for s in app.stream(initial_state):\n",
        "                pass # Prints are handled within the nodes\n",
        "\n",
        "            # Get the final state after execution (the last state from the stream)\n",
        "            final_state = app.invoke(initial_state)\n",
        "            print(\"\\n\" + \"=\"*25 + \" FINAL RESULT \" + \"=\"*25)\n",
        "            print(\"Workflow execution completed.\")\n",
        "            if final_state[\"final_result\"]:\n",
        "                print(\"Result Summary:\")\n",
        "                print(final_state[\"final_result\"])\n",
        "            else:\n",
        "                print(\"No specific result generated for this query. The agent may have determined no action was needed or encountered an issue.\")\n",
        "            print(\"=\"*64 + \"\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n[ERROR] An unexpected error occurred during workflow execution: {e}\")\n",
        "            print(\"Please ensure your API keys are correctly configured and try another query.\")\n",
        "\n",
        "\n",
        "run_agent_workflow_from_user_input()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qXs0QnEQyMF",
        "outputId": "64648ab2-45d3-4067-db65-5a20e4c34798"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API keys loaded from Google Colab secrets.\n",
            "--- Agentic Workflow powered by Langgraph (Real LLM & Tools) ---\n",
            "Enter your queries below. Type 'exit' to quit.\n",
            "============================================================\n",
            "\n",
            "Enter your query: what is machine learning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
            "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 52\n",
            "}\n",
            "].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[SYSTEM] Processing query: 'what is machine learning'\n",
            "\n",
            "--- PlanAgent (Iteration 0) ---\n",
            "User Query: what is machine learning\n",
            "Existing Sub-tasks: []\n",
            "Reflection from previous step: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
            "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 50\n",
            "}\n",
            "].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error invoking PlanAgent LLM: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
            "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 50\n",
            "}\n",
            "]. Falling back to default plan.\n",
            "PlanAgent generated/updated sub-tasks: [{'id': 'task_1', 'description': 'Address the query: what is machine learning'}]\n",
            "\n",
            "--- Deciding Next Step from PlanAgent ---\n",
            "Tasks planned: 1, Current task index: 0\n",
            "Decision: Tasks available. Continue to ToolAgent for execution.\n",
            "\n",
            "--- ToolAgent ---\n",
            "Executing task (1/1): Address the query: what is machine learning\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
            "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 47\n",
            "}\n",
            "].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in ToolAgent execution: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
            "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 48\n",
            "}\n",
            "]\n",
            "Task result: Error during tool execution for task 'Address the query: what is machine learning': 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
            "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 48\n",
            "}\n",
            "]\n",
            "\n",
            "--- Reflection Node ---\n",
            "Reflection: All planned tasks processed.\n",
            "\n",
            "--- Deciding Next Step from Reflection ---\n",
            "Current reflection: 'All planned tasks processed. Ready for final review.'\n",
            "Current task index: 1, Total tasks: 1\n",
            "Decision: All tasks completed. END workflow.\n",
            "\n",
            "--- PlanAgent (Iteration 0) ---\n",
            "User Query: what is machine learning\n",
            "Existing Sub-tasks: []\n",
            "Reflection from previous step: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
            "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 45\n",
            "}\n",
            "].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error invoking PlanAgent LLM: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
            "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 45\n",
            "}\n",
            "]. Falling back to default plan.\n",
            "PlanAgent generated/updated sub-tasks: [{'id': 'task_1', 'description': 'Address the query: what is machine learning'}]\n",
            "\n",
            "--- Deciding Next Step from PlanAgent ---\n",
            "Tasks planned: 1, Current task index: 0\n",
            "Decision: Tasks available. Continue to ToolAgent for execution.\n",
            "\n",
            "--- ToolAgent ---\n",
            "Executing task (1/1): Address the query: what is machine learning\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "Error in ToolAgent execution: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
            "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 43\n",
            "}\n",
            "]\n",
            "Task result: Error during tool execution for task 'Address the query: what is machine learning': 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
            "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 43\n",
            "}\n",
            "]\n",
            "\n",
            "--- Reflection Node ---\n",
            "Reflection: All planned tasks processed.\n",
            "\n",
            "--- Deciding Next Step from Reflection ---\n",
            "Current reflection: 'All planned tasks processed. Ready for final review.'\n",
            "Current task index: 1, Total tasks: 1\n",
            "Decision: All tasks completed. END workflow.\n",
            "\n",
            "========================= FINAL RESULT =========================\n",
            "Workflow execution completed.\n",
            "Result Summary:\n",
            "\n",
            "- Address the query: what is machine learning: Error during tool execution for task 'Address the query: what is machine learning': 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
            "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-pro\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 43\n",
            "}\n",
            "]\n",
            "================================================================\n",
            "\n",
            "\n",
            "Enter your query: exit\n",
            "Exiting workflow. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XAVmpaFKB6IV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}